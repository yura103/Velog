# [ML] 지도학습 - 회귀(1): 단일 모델

- Date: 2026-02-24 07:06:41 UTC
- Velog: https://velog.io/@yura103/ML-%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-%ED%9A%8C%EA%B7%801-%EB%8B%A8%EC%9D%BC-%EB%AA%A8%EB%8D%B8

---

<h3 id="1-회귀regression">1. 회귀(Regression)</h3>
<p>: 출력값이 연속형 숫자(value)인 지도학습</p>
<ul>
<li>입력 x로부터 예측값 ŷ를 만드는 과정<ul>
<li>집의 특성 x(면적, 역세권, 층수) -&gt; 예측 집값 ŷ=7.3억</li>
<li>내일 날씨 특징 x(기압, 습도) -&gt; 예측 기온 ŷ=28.4°C</li>
</ul>
</li>
<li>종류<ul>
<li>입력 기준<ul>
<li>단변량 회귀: 입력 x가 1개</li>
<li>다변량 회귀: 입력 x가 여러 개</li>
</ul>
</li>
<li>출력 개수 기준<ul>
<li>단일 출력 회귀: 예측값 y가 하나</li>
<li>다중 출력 회귀: 한 번에 여러 개의 숫자 예측</li>
</ul>
</li>
<li>출력 분포 기준<ul>
<li>일반 연속값: y가 연속형 숫자(온도, 키, 점수)</li>
<li>카운트 데이터(Poisson Regression): y가 0개 이상 정수<ul>
<li>음수가 나올 수 없고 평균이 커질수록 분산도 커짐</li>
<li>λ=exp(w⋅x)</li>
<li>예) 주문 건수, 방문 수, 사고 발생 횟수</li>
</ul>
</li>
<li>금액형 데이터(Tweedie): 대부분이 0인 데이터<ul>
<li>발생하면 양수 연속값이고 분산이 매우 큼</li>
<li>Poisson + Gamma 성질을 섞은 분포</li>
<li>예) 보험 청구 금액, 광고 클릭 후 결제 금액, 손해액</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-손실함수-선택">1) 손실함수 선택</h4>
<ul>
<li>MAE(L1): |y - ŷ|<ul>
<li>이상치에 덜 흔들리고 큰 오차도 선형으로 벌점</li>
<li>중앙값 쪽으로 예측을 끌어당기는 성질</li>
</ul>
</li>
<li>MSE(L2): (y - ŷ)²<ul>
<li>큰 오차에 매우 민감(제곱 벌점)</li>
<li>&quot;평균&quot;쪽으로 예측을 끌어당기는 성질</li>
</ul>
</li>
<li>RMSE: √MSE<ul>
<li>단위가 원래 y 단위로 돌아와서 해석 쉬움</li>
</ul>
</li>
<li>Huber loss: 작은 오차는 L2처럼, 큰 오차는 L1처럼<ul>
<li>평소에는 MSE의 매끈함과 이상치에는 MAE의 강인함으로 타협<h6 id="큰-오차-한-번이-치명적이면-msermse-쪽">큰 오차 한 번이 치명적이면 MSE/RMSE 쪽</h6>
<h6 id="데이터에-이상치라벨-노이즈-많으면-maehuber-쪽">데이터에 이상치/라벨 노이즈 많으면 MAE/Huber 쪽</h6>
</li>
</ul>
</li>
</ul>
<h4 id="2-target-스케일분포">2) Target 스케일/분포</h4>
<ul>
<li>문제 상황: 0 ~ N억(집값), long-tail(매출), 양수만 존재, 분산 커짐</li>
<li>해결: log1p 변환(y’ = log(1 + y))<ul>
<li>큰 값의 영향 줄이고, &quot;비율 오차&quot;에 가까운 최적화가 됨</li>
<li>예측 후에는 expm1로 되돌림</li>
<li>log 변환은 0/음수 처리를 설계해야 함(0이면 log1p, 음수면 다른 변환/모델링 필요)</li>
</ul>
</li>
</ul>
<h4 id="3-평가지표">3) 평가지표</h4>
<ul>
<li>R²: 1 - (SSE/SST)<ul>
<li>평균 예측 대비 얼마나 설명했는지</li>
<li>데이터 스케일/분포 따라 착시 있음</li>
</ul>
</li>
<li>MAE/RMSE</li>
<li>MAPE: |(y-ŷ)/y|<ul>
<li>y가 0 근처면 폭발, 작은 값에 과민 -&gt; 조심</li>
</ul>
</li>
<li>SMAPE: 비율형 보완<br />

</li>
</ul>
<h3 id="2-단일-모델baseline">2. 단일 모델(Baseline)</h3>
<h4 id="1-선형-회귀linear-regression">1) 선형 회귀(Linear Regression)</h4>
<p>로지스틱 회귀랑 똑같이 점수 만드는 구조 (다만, 분류처럼 sigmoid 확률로 바꾸지 않고 그 점수가 곧 예측값)</p>
<ul>
<li>ŷ = w·x + b</li>
<li>보통 MSE 최소화가 되게 w, b를 찾음</li>
<li>장점: 빠르고 해석 가능, baseline으로 최고</li>
<li>단점: 비선형 관계 못 잡음, 이상치에 흔들림</li>
</ul>
<h4 id="2-ridgelassoelastic-net정규화-회귀">2) Ridge/Lasso/Elastic Net(정규화 회귀)</h4>
<p>회귀에서는 다중공선성(피처끼리 비슷함)이 자주 터지는데 Ridge가 안정적으로 잡아줌.</p>
<ul>
<li>Ridge(L2): 가중치 전체를 조금씩 줄여 안정화</li>
<li>Lasso(L1): 일부 가중치를 0으로 -&gt; 피처 선택</li>
<li>ElasticNet(L1+L2): 둘 섞어서 타협<blockquote>
<p>피처 많고 상관 강하면 Ridge/ElasticNet이 편함
&quot;설명 가능한 소수 피처&quot;를 뽑고 싶으면 Lasso가 유용</p>
<br />
</blockquote>
</li>
</ul>
<h4 id="3-svr회귀-버전-svm">3) SVR(회귀 버전 SVM)</h4>
<p>: 예측선 주변에 '허용 오차 폭 ε’(엡실론)을 두고, 그 안의 오차는 &quot;0으로 치고&quot; 무시하면서 모델을 최대한 단순하게(너무 요동치지 않게) 만드는 회귀
<img alt="image" src="https://velog.velcdn.com/images/yura103/post/9ebddb72-1c22-4f50-8651-507d64d133a9/image.png" width="500" /></p>
<ul>
<li>ε 오차 - ε-insensitive loss<ul>
<li>회귀에서 보통 오차가 0이 아니면 무조건 벌점인데 작은 오차까지 다 맞추려다 모델이 흔들릴 수 있으니, 엡실론 안쪽 오차는 벌점 0으로 함.</li>
<li>예시 (ε=2)<ul>
<li>정답 100, 예측 101 -&gt; 오차 1 (ε 안) -&gt; 벌점 0</li>
<li>정답 100, 예측 98 -&gt; 오차 2 (ε 안) -&gt; 벌점 0</li>
<li>정답 100, 예측 95 -&gt; 오차 5 (ε 밖) -&gt; 벌점 발생 (5-2=3만큼만)</li>
</ul>
</li>
</ul>
</li>
<li>ε-튜브: 예측함수 ŷ(x) 주위로 위아래 ε만큼 띠(튜브)를 만든다고 생각하면 됨.<ul>
<li>튜브 안: &quot;맞춘 걸로 치자&quot;</li>
<li>튜브 밖: 벗어난 거리만큼만 벌점  </li>
</ul>
</li>
<li>핵심 파라미터<ul>
<li>ε: 허용 오차 폭<ul>
<li>커지면: 더 많이 무시(관대) -&gt; 매끈하지만 디테일 놓침(편차 큼)</li>
<li>작아지면: 더 엄격 -&gt; 디테일 맞추려다 흔들릴 수 있음(분산 큼)</li>
</ul>
</li>
<li>C: 튜브 밖 점에 대한 벌점 세기<ul>
<li>커지면: &quot;튀어나온 점 무조건 맞춰!&quot; -&gt; 과적합 위험 증가</li>
<li>작아지면: &quot;좀 틀려도 돼&quot; -&gt; 더 부드러워짐. 과소적합 위험 증가<h5 id="종류">종류</h5>
<table>
<thead>
<tr>
<th>구분</th>
<th>Linear SVR</th>
<th>Kernel SVR</th>
</tr>
</thead>
<tbody><tr>
<td>관계 표현</td>
<td>선형</td>
<td>비선형 가능</td>
</tr>
<tr>
<td>계산량</td>
<td>비교적 빠름</td>
<td>데이터 커질수록 급격히 증가</td>
</tr>
<tr>
<td>커널 행렬</td>
<td>사용 안 함</td>
<td>N×N 계산</td>
</tr>
<tr>
<td>확장성</td>
<td>큰 데이터에도 비교적 가능</td>
<td>대규모 데이터 부적합</td>
</tr>
<tr>
<td>튜닝 난이도</td>
<td>C, ε 중심</td>
<td>C, ε, gamma 등 더 복잡</td>
</tr>
<tr>
<td>추천 상황</td>
<td>피처 많고 선형 근사 가능할 때</td>
<td>데이터 적고 복잡한 곡선 필요할 때</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="svr-쓰는-상황">SVR 쓰는 상황</h5>
<table>
<thead>
<tr>
<th>구분</th>
<th>좋은 상황</th>
<th>안 좋은 상황</th>
</tr>
</thead>
<tbody><tr>
<td>데이터 크기</td>
<td>데이터가 아주 크지 않음 (수만~수십만 이하)</td>
<td>데이터가 매우 큼 (특히 커널 SVR)</td>
</tr>
<tr>
<td>데이터 특성</td>
<td>노이즈가 있고 작은 오차는 중요하지 않음</td>
<td>작은 오차까지 정밀하게 맞춰야 함</td>
</tr>
<tr>
<td>관계 구조</td>
<td>비선형 구조가 있지만 트리 말고 다른 해법도 보고 싶을 때</td>
<td>고차원 sparse 데이터에서 커널 사용 시 비효율</td>
</tr>
<tr>
<td>모델 특성</td>
<td>매끈하고 안정적인 함수 형태를 원할 때</td>
<td>튜닝(ε, C, gamma)이 까다로워 빠른 실험이 어려울 때</td>
</tr>
<tr>
<td>계산 자원</td>
<td>중소 규모 실험, 연구용</td>
<td>대규모 실시간 시스템</td>
</tr>
</tbody></table>
<h5 id="필수-전처리">필수 전처리</h5>
<table>
<thead>
<tr>
<th>항목</th>
<th>설명</th>
</tr>
</thead>
<tbody><tr>
<td>스케일링</td>
<td><strong>필수에 가깝다.</strong> SVR은 내적/거리 기반이므로 피처 스케일에 매우 민감. StandardScaler/MinMaxScaler 권장</td>
</tr>
<tr>
<td>이상치 확인</td>
<td>ε-튜브 밖 점에 벌점이 가므로 극단값이 많으면 C 조정 필요</td>
</tr>
<tr>
<td>커널 선택</td>
<td>Linear → 단순/빠름, RBF → 비선형/느림, gamma 조정 필수</td>
</tr>
<tr>
<td><br /></td>
<td></td>
</tr>
</tbody></table>
<h4 id="4-knn-regression">4) KNN Regression</h4>
<ul>
<li>분류가 다수결이면, 회귀는 주변 k개 이웃의 y를 평균(가중평균)해서 예측<ul>
<li>가중평균: 거리가 가까운 이웃에 더 큰 가중치(거리 역수 등)</li>
</ul>
</li>
<li>데이터가 많아질수록 예측 느림</li>
<li>고차원에서 급격히 망가짐</li>
<li>스케일링 중요<ul>
<li>feature별 단위 다르면 StandardScaler 써서 표준화
<img alt="" src="https://velog.velcdn.com/images/yura103/post/0d50abf4-7e85-4382-9e99-d7888166cf92/image.png" /></li>
</ul>
</li>
</ul>
<h4 id="5-decision-tree-regressor">5) Decision Tree Regressor</h4>
<ul>
<li>분류 트리는 지니/엔트로피로 &quot;라벨 섞임&quot; 최소화였고, 회귀 트리는 보통 분기 후 MSE 감소(분산 감소)가 최대가 되게 자름</li>
<li>리프의 예측값: 그 리프에 들어온 y들의 평균(MSE 기준)</li>
<li>트리 회귀는 특히 계단형 예측이 나옴(구간마다 일정한 값)</li>
</ul>
<table>
<thead>
<tr>
<th>문제</th>
<th>원인</th>
<th>해결책</th>
</tr>
</thead>
<tbody><tr>
<td>과적합</td>
<td>노이즈까지 완벽 외움 (훈련 MSE=0)</td>
<td>max_depth: 깊이 제한</td>
</tr>
<tr>
<td></td>
<td>각 샘플마다 리프 생성</td>
<td>min_samples_leaf: 리프당 최소 샘플 수</td>
</tr>
<tr>
<td></td>
<td>테스트셋 성능 급락</td>
<td>앙상블 (Random Forest 등)</td>
</tr>
</tbody></table>
